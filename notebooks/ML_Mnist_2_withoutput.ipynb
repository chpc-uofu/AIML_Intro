{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191ce7a7-8fbc-4ea6-8eab-11f6bf224fd8",
   "metadata": {},
   "source": [
    "# MNIST Dataset \n",
    "\n",
    "* Dataset used to prove image recognition can be done with machine learning\n",
    "* Images are grayscale 28 x 28\n",
    "* Digits 0-9\n",
    "* Goal here is OCR (Optical Character Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80f28f-4189-4718-a0ee-a764ef24f4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports - tensorflow, numpy (a numerical library very common in scientific applications) and plotting library.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from AccuracyHistory import AccuracyHistory, MSEHistory #locally defined class, don't worry about the workings of this now\n",
    "tf.random.set_seed(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da039509-07aa-4365-b051-c736708e0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST is a classic learning dataset and is provided as part of tensorflow.\n",
    "# It will be automatically downloaded if you do not have it already.\n",
    "mnist = keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38adec-6be0-43f4-b46f-51f975d168fc",
   "metadata": {},
   "source": [
    "# Train and test partition split\n",
    "\n",
    "* As mentioned before, this must be done to properly judge model performance\n",
    "* MNIST knows you want to do this\n",
    "* Data preprocessing is done for you here but can be a very manual and time consuming process\n",
    "* Presentation with library about data handling coming in spring, it can be very involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c24f1-f995-404d-a4f1-d408ac776b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e44cdd-9a8f-4968-ab2b-4d565c500041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUT WHAT DO THEY LOOK LIKE? I looked at mnist and found indexes with a sample of each digit\n",
    "numerical_indexes = [1,8,122,111,9,332,4572,223,333,1333]\n",
    "# Finally we will answer what these images look like.\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10,10))\n",
    "for n in range(0,len(numerical_indexes)):\n",
    "    axes[n].imshow(train_images[numerical_indexes[n]],  cmap=plt.cm.binary)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd89e1b-2f31-431f-81bf-1336f899c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's print 10 variations of 5\n",
    "variations = 0\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10,10))\n",
    "for n in range(0,60000):\n",
    "    if train_labels[n] == 5:\n",
    "         axes[variations].imshow(train_images[n], cmap=plt.cm.binary)\n",
    "         variations += 1\n",
    "    if variations == 10:\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480af5b1-4759-4f33-b6b5-f685277ee854",
   "metadata": {},
   "source": [
    "# Is dataset balanced?\n",
    "\n",
    "* By which we mean, are all classes represented equally?\n",
    "* A dataset that is imbalanced can lead to many different problems.\n",
    "* e.g. You are doing binary classification so your classes are 0 or 1. Your dataset is 95% 0.\n",
    "* A model can get a very good score just by predicting 0 all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e581f1-09a9-4b02-b4d4-c98b695f0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.zeros(10) #MNIST is for digits 0-9 for total of 10\n",
    "for i in range(len(train_labels)):\n",
    "    class_counts[train_labels[i]] += 1\n",
    "for i in range(len(class_counts)):\n",
    "    print(i, \":\", str(class_counts[i] / len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2abdce-5d98-4101-af18-fd6ec62b50d2",
   "metadata": {},
   "source": [
    "# Not perfectly balanced but close enough.\n",
    "\n",
    "# Class question: What is close enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880949c-1716-4322-983f-449497d0c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small preprocessing note - we want to scale image values to between 0 and 1\n",
    "print(train_images[333])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c10a2-3781-43df-853f-51897246efe2",
   "metadata": {},
   "source": [
    "# Exploding gradient problem - scale our data to between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e2418-0898-46ad-b8cc-4e7dd53f1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the global minimum and maximum pixel values\n",
    "min_val = train_images.min()\n",
    "max_val = train_images.max()\n",
    "\n",
    "# Apply the MinMax formula to the entire array\n",
    "train_images = (train_images - min_val) / (max_val - min_val)\n",
    "\n",
    "# Optional: Print to verify the new range\n",
    "print(f\"Original min: {min_val}, max: {max_val}\")\n",
    "print(f\"New max: {train_images.max()}\")\n",
    "\n",
    "# Now do same for test\n",
    "test_images = (test_images - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052050c-55f8-4793-8e81-f9f194c8a477",
   "metadata": {},
   "source": [
    "# Side note:\n",
    "\n",
    "* We are going to change the shape of our images so we can use a special plotting library at the end\n",
    "* This library requires images to have 3 \"dimensions.\"\n",
    "* MNIST images are 28x28 but we are going to reshape them to 28x28x1\n",
    "* This is because most pictures used in convolutional neural networks have 3 values per pixel (RGB)\n",
    "* MNIST images are grayscale so only have a single value per pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0960c0a-bc2d-4164-af7b-c3659729d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before reshape\", train_images.shape)\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "print(\"After reshape\", train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120ea3d-4984-4c59-a2d0-9b9a46483419",
   "metadata": {},
   "source": [
    "# Define model\n",
    "\n",
    "* Recall diagram - Models are made of multiple layers, each of which contains a number of units.\n",
    "* How many layers? How many hidden units in each layer? \n",
    "* Some tips to start: keep it simple. Simpler networks generalize better (similar performance to train and test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb916fd7-21e3-43eb-a5ad-8da20039fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is basically one line in Keras\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426be203-290d-4302-ba18-9153922af7f9",
   "metadata": {},
   "source": [
    "# Defining First layer\n",
    "\n",
    "<img src=\"../images/Backprop.jpg\" width=\"400\" height=\"400\" align=right />\n",
    "\n",
    "* First layer must be equal in size to an individual item in your dataset.\n",
    "* We will specify the 28 x 28 resolution of each individual image in mnist.\n",
    "* Flatten here is really important - it expands 28 x 28 into 784 consecutive numbers - this could be any kind of data, not just pictures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba4295-bc57-40f4-89f7-a9573ca03762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten(input_shape=train_images[0].shape)) \n",
    "# MNIST images are 28 x 28, Flatten turns this into 1-D array of 784 (28 x 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaec16-c542-4e64-868c-1149ce4e64f9",
   "metadata": {},
   "source": [
    "# Defining Second layer\n",
    "\n",
    "* The number of hidden units should be between the size of the input layer and the size of the output layer\n",
    "* If you want an exact formula, sadly there isn't one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0faf3-ff22-4881-8ed3-e05107c6ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc199d0-7646-413c-9f72-64565b48cf9f",
   "metadata": {},
   "source": [
    "# Third layer\n",
    "\n",
    "* For MNIST, this will be the number of categories of our target. How many do we have?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc554a-8246-44f6-98fb-f5e8014fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f49ef-d13a-47e1-955d-031477f10a7b",
   "metadata": {},
   "source": [
    "# Output Layer\n",
    "\n",
    "* This network will output a series of raw prediction numbers (aka logits) for each class\n",
    "* Q: What if we want probabilities rather than raw numbers?\n",
    "* A: Add a softmax layer to this. We will do that later in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb9285-37e2-40af-bb92-d46f9978d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is basically one line in Keras\n",
    "model.add(keras.layers.Softmax())  # Converts raw math numbers to human-readable probabilities that sum to \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e1762-36d5-44f0-b11b-3f7867c2334d",
   "metadata": {},
   "source": [
    "# In Tensorflow, creating model topology is the first step\n",
    "\n",
    "* Now we have to deal with the other components I mentioned\n",
    "\n",
    "#  Pop quiz - what were they?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271e8d4-9025-492b-8be9-003577470895",
   "metadata": {},
   "source": [
    "# Loss function \n",
    "* If your data is all 0s or 1s, you would use BinaryCrossentropy\n",
    "* If you have multiple classes, you would want either CategoricalCrossentropy or SparseCategoricalCrossentropy\n",
    "* If you were doing regression, you would probably use MeanSquaredError\n",
    "\n",
    "# What do our targets really look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3305c-1015-4355-9044-71b03d163f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of targets do we have and how are they encoded?\n",
    "np.unique(train_labels)\n",
    "# What should we use from the above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97d08c-ba95-4ae0-84b6-e1b6b7c7c28f",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "* Because the labels are from 0-9, this means we should use SparseCategoricalCrossentropy\n",
    "* If label looked like [0,0,1,0,0,0,0,0,0,0] indicating 2 (also known as one-hot encoding), \n",
    " we would use CategoricalCrossentropy.\n",
    "* Next step after model creation is compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83f30d-b920-40bd-9638-3ddd550bd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b796984-a5e0-4890-82eb-f30ac8930bb6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Epochs\n",
    "\n",
    "* Epochs are how many times we do the training loop from the diagram above\n",
    "* This has a dramatic effect on training time\n",
    "* if you don't curtail it with EarlyStopping, which we will discuss briefly but not implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579c72a-9a93-494b-96ab-10325100def2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=3\n",
    "history=AccuracyHistory() #User made class we will discuss later\n",
    "model.fit(train_images, train_labels, epochs=epochs, callbacks=[history]) # fit means train, old terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7c193-19d0-4213-8edb-0f00f66e7d6a",
   "metadata": {},
   "source": [
    "# Accuracy sidebar\n",
    "\n",
    "* for classification, what is good enough?\n",
    "\n",
    "* Baseline aka the floor - what would a bad model predict if it predicted the same thing every time and didn't learn anything?\n",
    "\n",
    "# Class question: what is the baseline accuracy for MNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949ff63-5aea-4225-b2dd-c2326bf84d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot accuracy and loss graphs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "axes[0].plot(range(1, epochs + 1), history.acc)\n",
    "axes[0].set_title('Accuracy')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Training Set Accuracy')\n",
    "\n",
    "axes[1].plot(range(1, epochs + 1), history.loss)\n",
    "axes[1].set_title('Loss')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372301ff-e0a0-4c5f-8c37-3fc65a81f90c",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "* MNIST is a simple problem that was pretty well learned after 2 epochs\n",
    "* It gives us the classic graphs of what SHOULD happen - accuracy goes up over time and loss goes down over time\n",
    "* Too much training can result in overfitting\n",
    "* your model's training performance looks good but it does not generalize well on test data.\n",
    "\n",
    "# Speaking of which, time for the moment of truth\n",
    "\n",
    "* How well does this model do on test data it has not seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99130814-7b63-477d-91bf-ab100c79902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "#Plot accuracy and loss graphs\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(range(1, epochs + 1), history.acc)\n",
    "plt.plot([epochs], [test_acc], color='green', marker='o')\n",
    "plt.title('Accuracy (train in blue, test in green)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d371d-48e6-4e9f-81ce-211e835ebfe3",
   "metadata": {},
   "source": [
    "# Looks close!\n",
    "\n",
    "* If this number was significantly lower than our training accuracy, that would indicate overfitting.\n",
    "\n",
    "* Define significant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0795407-0298-428d-b1e2-f1a02fa5e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do predictions look like?\n",
    "np.set_printoptions(suppress=True)\n",
    "predictions = model.predict(test_images)\n",
    "print(predictions[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949d120-1e4b-4f11-95f6-ab534f2a5e3f",
   "metadata": {},
   "source": [
    "# Probabilities are more human readable\n",
    "\n",
    "* Probabilities for each class are good for seeing where your model might be getting confused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c0791-df62-4092-a053-72f66996fbbc",
   "metadata": {},
   "source": [
    "# Black Box problem\n",
    "\n",
    "* For years, machine learning has been criticized for being a black box. \n",
    "* Seeing a probability is nice but in many cases, we will want to know why the model decided as it did. \n",
    "* For instance, there can be discrimination possibilities if a model uses certain information.\n",
    "* Legally protected information should be left out of a model entirely but as we see with AI, many times questionable sources are used in model training.\n",
    "\n",
    "# Shapley values to the rescue\n",
    "\n",
    "* Shapley values use permutation-based game theory to open the black box and see why a model decided as it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b3f40-1689-4669-b264-614ff735595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "background = train_images[np.random.choice(train_images.shape[0], 4000, replace=False)]\n",
    "e = shap.DeepExplainer(model, background) # train_images)\n",
    "# Get the values. Ranked_outputs = 2 means the two highest ranked possible classes for each image.\n",
    "shap_values = e.shap_values(test_images[1:5])\n",
    "# plot the feature attributions\n",
    "# Red pixels represent positive SHAP values that increase the probability of the class, \n",
    "# while blue pixels represent negative SHAP values the reduce the probability of the class\n",
    "shap.image_plot(shap_values, test_images[1:5]) #, index_names)\n",
    "predictions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c4304-84a0-41c9-be79-80bf532f036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But what if some of your data is bad? Let's look at a randomly generated image.\n",
    "randomImg = np.array([np.random.rand(28,28)])\n",
    "randomImg = randomImg.reshape(28,28,1)\n",
    "print(randomImg.shape)\n",
    "randomImg = np.array([randomImg])\n",
    "random_shapvalues = e.shap_values(randomImg)\n",
    "shap.image_plot(random_shapvalues, randomImg)\n",
    "print(model.predict(randomImg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d512587-1b4f-4292-b9a9-23c075594e71",
   "metadata": {},
   "source": [
    "# If you're a hammer...\n",
    "\n",
    "<BR><BR><BR><BR><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd8a8b-e211-452d-9ade-98006e091f63",
   "metadata": {},
   "source": [
    "# What if we do something wrong?\n",
    "* MNIST is categorical but what if we treated it like regression?\n",
    "\n",
    "* Categorical - no category is higher/lower/comparable to any other category. They are independent.\n",
    "\n",
    "* Regression - 1 is close to 2. 1 is far from 9.\n",
    "\n",
    "# CLASS QUESTION - How many units for output layer of the network? We had 10 before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f53ee3-1a13-4235-a789-00cc4edfae25",
   "metadata": {},
   "source": [
    "# Problems you might have\n",
    "\n",
    "* One of the problems above - bad data, incorrect choices\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c403e3-8d54-403d-a726-7ec698faa9bd",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "* good training performance but bad test performance\n",
    "\n",
    "* Decrease complexity of model (units, layers, features, etc.), early stopping, cross-validation, batch normalization..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459760d-9e0f-46e9-838f-58156e26e538",
   "metadata": {},
   "source": [
    "# Underfitting\n",
    "\n",
    "* Bad training and test performance\n",
    "\n",
    "* Opposite of above - increase model complexity, get more data if possible (both quantity and quality), more epochs\n",
    "\n",
    "* If classification, check probabilities. Maybe model is getting confused for only a couple of categories and doing well on most of them.\n",
    "\n",
    "* Balance dataset with 0s and 1s if applicable? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21734f-e6e7-4cde-b82b-b1e340a5f5b8",
   "metadata": {},
   "source": [
    "# Class question: you have 10 categories like MNIST. Your model gets most categories right but keeps confusing 2.\n",
    "\n",
    "# What can you do?\n",
    "\n",
    "<details>\n",
    "  <summary>Click to reveal hidden text!</summary>\n",
    "  <p>Make another model that only has those 2 questionable classes. Maybe it can do better without the confusion of the rest.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6ee3e-e531-426f-b0ff-38adc54f5138",
   "metadata": {},
   "source": [
    "<img src=\"../images/mixingboard.jpg\" width=\"450\" height=\"400\" align=right />\n",
    "\n",
    "# Hyperparameters - the last refuge...\n",
    "\n",
    "* Learning rate, optimizer choice, activation function, many more we haven't mentioned\n",
    "  \n",
    "* Hyperparameters optimization – is “because it works” good enough?\n",
    "\n",
    "* Write your own functions (activation, loss, etc.)\n",
    "\n",
    "* Impossible tasks – AI/ML can’t do everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1b3f2-ef38-4a45-b0ab-c28a9cb6f1ad",
   "metadata": {},
   "source": [
    "# REMINDER - this is introductory class. If you want more details about anything here, watch for the next presentation in this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cda5a-4828-407a-8b73-42ee234cbe85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
